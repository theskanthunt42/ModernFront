<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>How did I run SakuraLLM with my Intel Arc B580 on Windows | the42Front</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="How did I run SakuraLLM with my Intel Arc B580 on Windows" />
<meta name="author" content="the42game" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="SakuraLLM(Qwen2.5) and generally llama.cpp on Intel Arc hardware HOWTO" />
<meta property="og:description" content="SakuraLLM(Qwen2.5) and generally llama.cpp on Intel Arc hardware HOWTO" />
<link rel="canonical" href="https://front.the42.info/2025/12/20/sakurallm-and-arc-b580.html" />
<meta property="og:url" content="https://front.the42.info/2025/12/20/sakurallm-and-arc-b580.html" />
<meta property="og:site_name" content="the42Front" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-12-20T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="How did I run SakuraLLM with my Intel Arc B580 on Windows" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"the42game"},"dateModified":"2025-12-20T00:00:00+08:00","datePublished":"2025-12-20T00:00:00+08:00","description":"SakuraLLM(Qwen2.5) and generally llama.cpp on Intel Arc hardware HOWTO","headline":"How did I run SakuraLLM with my Intel Arc B580 on Windows","mainEntityOfPage":{"@type":"WebPage","@id":"https://front.the42.info/2025/12/20/sakurallm-and-arc-b580.html"},"url":"https://front.the42.info/2025/12/20/sakurallm-and-arc-b580.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://front.the42.info/feed.xml" title="the42Front" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">the42Front</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/friends-of-the42/">Friends</a><a class="page-link" href="/motw/">MotW</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">How did I run SakuraLLM with my Intel Arc B580 on Windows</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2025-12-20T00:00:00+08:00" itemprop="datePublished">Dec 20, 2025
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">the42game</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="tldr-and-contexts-as-always">TL;DR and contexts as always:</h1>

<p><a href="https://github.com/SakuraLLM/SakuraLLM">SakuraLLM</a> is a LLM model trained specifically for translating Japanese ACGN contents (LNs and VNs) to Chinese, based on Alibaba’s <a href="https://github.com/QwenLM/Qwen">Qwen</a>.</p>

<p>And are mostly use with the amazing easy to use tools provided by <a href="https://n.novelia.cc">auto-novel</a> (<a href="https://github.com/auto-novel/auto-novel">Github</a>) to translate Japanese Light Novels (that are being serialize online) into Chinese, at least for me.</p>

<p>And for something with such a massive(and also niche) audiences, there must be some easy to use setup tools, which there are: Some rather powerful and easy to use tools like <a href="https://github.com/PiDanShouRouZhouXD/Sakura_Launcher_GUI">Sakura Launcher GUI</a> and <a href="https://github.com/neavo/OneClickLLAMA">OneClickLLAMA</a>.</p>

<p>With the support centers around NVIDIA’s CUDA, which is the dominant of GPU market as of the time of writing, with additional supports for APIs such as AMD/ATI’s ROCm, Apple’s Metal, and even if you have none of those, fallback to VULKAN for a wider range of support (Everything after Intel HD4000 got some kinds of Vulkan supports anyway…), but using Vulkan as the API might hinder some of the performance.</p>

<p>As a huge fan of Yuri-related contents in Japanese (Since 2014), that still can’t be able to read too complex Japanese, I sure have a agenda to fill which is to run this LLM locally, without waiting others to run it for me.</p>

<p>With tight budget and constant look out on bargain pricing (and my anti-nvidia tradition) in mind, I went for an Intel Arc B580 GPU instead, the best GPU you could get for the price.</p>

<p>But, Santa Clara, we had a problem.</p>

<h2 id="whats-the-problem">Whats the problem?</h2>

<p>Well as you can tell, Intel’s Arc was pretty new at this point, so there aren’t too many supports arounds it, and the financial struggles sure didn’t help that.</p>

<p>So in the end, the easiest way for me to run it is via Vulkan, but I don’t want to.</p>

<p>And there is <a href="https://github.com/intel/ipex-llm">IPEX-LLM</a>, an LLM acceleration library for Intel GPUs like Arc, which thanks to the Intel team behinds it, they provide intergration of llama.cpp and ollama.</p>

<p>For me, I am going to run <a href="https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/llamacpp_portable_zip_gpu_quickstart.md">llama.cpp</a> on Windows (Shout out to the Intel guys for providing a Portable version thats saves a lot of time) but it will also works on Linux as well. And the end results was pretty nice but require some level of care taking in the process.</p>

<h2 id="howto-starts-here">HOWTO STARTS HERE:</h2>

<p>Follow the guide till <a href="https://github.com/intel/ipex-llm/blob/main/docs/mddocs/Quickstart/llamacpp_portable_zip_gpu_quickstart.md">here</a>. Prepare yourself a .bat file in the same folder as your llama-cli/-serve.exe, don’t forget to download the actual .gguf files and put them into the same folder or any place you want! For Arc B580 I suggest you to use <a href="https://huggingface.co/SakuraLLM/Sakura-14B-Qwen2.5-v1.0-GGUF/blob/main/sakura-14b-qwen2.5-v1.0-iq4xs.gguf">sakura-14b-qwen2.5-v1.0-iq4xs.gguf</a> for saving on VRAMs (which fits right into the 12GB VRAM of the B580).</p>

<p>Open your whatever bat file and put:</p>

<div class="language-batch highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">set</span> <span class="kd">ONEAPI_DEVICE_SELECTOR</span><span class="o">=</span><span class="kd">level_zero</span>:0
<span class="kd">set</span> <span class="kd">ZES_ENABLE_SYSMAN</span><span class="o">=</span><span class="m">1</span>
<span class="kd">set</span> <span class="kd">SYCL_CACHE_PERSISTENT</span><span class="o">=</span><span class="m">1</span>
<span class="kd">set</span> <span class="kd">SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS</span><span class="o">=</span><span class="m">1</span>
<span class="kd">llama</span><span class="na">-server</span>.exe <span class="na">-m </span><span class="kd">PATH</span><span class="na">/TO/SAKURALLM</span>.GGUF <span class="na">-ngl </span><span class="m">47</span> <span class="na">-fa -c </span><span class="m">6194</span> <span class="na">-v -t </span><span class="m">20</span> <span class="na">-a </span><span class="kd">sakura</span><span class="o">-</span><span class="m">14</span><span class="kd">b</span><span class="na">-qwen</span><span class="m">2</span>.5<span class="na">-v</span><span class="m">1</span>.0<span class="na">-iq</span><span class="m">4</span><span class="kd">xs</span>.gguf <span class="na">--host </span><span class="m">0</span>.0.0.0 <span class="na">--port </span><span class="m">11434</span>
</code></pre></div></div>
<p>Change the PATH/TO/SAKURALLM.GGUF to your file path, save the file, and run this .bat file and you are DONE(in an good way). The only thing you would needs to do is to open <a href="https://n.novelia.cc">auto-novel</a> and do your thing.
if it complains about thread or sth like that set <code class="language-plaintext highlighter-rouge">-t</code> to something like <code class="language-plaintext highlighter-rouge">8</code></p>

<p>Lght, let me expalins all the necessary components of this bat file and what they do:</p>

<p><code class="language-plaintext highlighter-rouge">set ONEAPI_DEVICE_SELECTOR=level_zero:0</code>: If you have multiple Intel GPUs(including the iGPU inside your CPU), you should manually select which one to use, or you will face severe performance punishment due to the bandwidth between CPU and GPU ain’t all that great… Please refer to this part of the guide.</p>

<p>The recommended settings from devs in the README file in the zip, seems to enhance performance:</p>

<div class="language-batch highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">set</span> <span class="kd">ZES_ENABLE_SYSMAN</span><span class="o">=</span><span class="m">1</span>
<span class="kd">set</span> <span class="kd">SYCL_CACHE_PERSISTENT</span><span class="o">=</span><span class="m">1</span>
<span class="kd">set</span> <span class="kd">SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS</span><span class="o">=</span><span class="m">1</span>
</code></pre></div></div>

<p>And lastly:</p>

<div class="language-batch highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">llama</span><span class="na">-server</span>.exe <span class="na">-m </span><span class="kd">PATH</span><span class="na">/TO/SAKURALLM</span>.GGUF <span class="na">-ngl </span><span class="m">47</span> <span class="na">-fa -c </span><span class="m">6194</span> <span class="na">-v -t </span><span class="m">20</span> <span class="na">-a </span><span class="kd">sakura</span><span class="o">-</span><span class="m">14</span><span class="kd">b</span><span class="na">-qwen</span><span class="m">2</span>.5<span class="na">-v</span><span class="m">1</span>.0<span class="na">-iq</span><span class="m">4</span><span class="kd">xs</span>.gguf <span class="na">--host </span><span class="m">0</span>.0.0.0 <span class="na">--port </span><span class="m">11434</span>
</code></pre></div></div>

<p>Runs the actual LLAMA.CPP server itself, with <code class="language-plaintext highlighter-rouge">-ngl</code> set to <code class="language-plaintext highlighter-rouge">47</code>, <code class="language-plaintext highlighter-rouge">-c</code>(context) set to 6194, and had <code class="language-plaintext highlighter-rouge">-t</code> set to 20 (I am using a Intel i5-14600K here.), and let LLAMA-SERVER to tell REST API which model its using by <code class="language-plaintext highlighter-rouge">-a sakura-14b-qwen2.5-v1.0-iq4xs.gguf</code> change it accordingly to the actual model name you use</p>

<p>and lastly are the address and ports to run the server, change it to you liking.</p>

<h2 id="some-performance-figures">Some performance figures.</h2>

<p>I’ve gather something around 10 to 12 tokens/s with i5-14600K plus 2*32GB DDR4 RAM and Intel Arc B580 itself.</p>

<p>And please remember never let it running out of VRAM, I’VE SUFFERED FRIN SEVERE PERFORMANCE PENALTY FOR THAT AND I SPENT A WHOLE NIGHT TO FIGURE OUT THAT.</p>

<p>At last, huge thanks to Intel Arc’s team, you guys are amazing for providing something like this. And keep up the good work Intel, we really need someone to compete with Nvidia and AMD(albeit AMD barely did anything tbh… I am really disappointed).</p>

<p>And ALSO HUGE THANKS TO THE GUYS BEHIND SAKURALLM AND DEVS OF THE TOOLS AND WEBSITES, without them i might still be hard reading those lovely tasty yuri light novels in Japanese in pain…</p>

<p>P.S: I might re-write this whole things in Chinese later on for helping others, but lets face the facts, the one who will need this guide are either: the one who had a laptop with Intel Arc Graphics and tons of RAM in it, or geeks who already know how to do. So I hope I would get a Chinese version done if I had the time lol, for the sake of the former.</p>

<p>If you have any problem or suggestion to this article, please feel free to reach out to me on Telegram via <a href="https://front.the42.info/ch">here</a>, you know the drill, I am the only person in the Channel who could send posts.</p>

<p>Thank you for reading.</p>


  </div><a class="u-url" href="/2025/12/20/sakurallm-and-arc-b580.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">the42Front</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">the42game</li><li><a class="u-email" href="mailto:kabs@the42.info">kabs@the42.info</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/theskanthunt42"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">theskanthunt42</span></a></li><li><a href="https://www.twitter.com/42lzmr"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">42lzmr</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Rebuilds after rebuilds. This time modernize.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
